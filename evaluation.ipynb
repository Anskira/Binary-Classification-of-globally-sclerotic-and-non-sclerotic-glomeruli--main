{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":10103774,"sourceType":"datasetVersion","datasetId":6232206},{"sourceId":10125428,"sourceType":"datasetVersion","datasetId":6248261},{"sourceId":10126421,"sourceType":"datasetVersion","datasetId":6249014},{"sourceId":10126552,"sourceType":"datasetVersion","datasetId":6249107}],"dockerImageVersionId":30804,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import os\nimport sys\nimport torch\nimport torch.nn as nn\nimport pandas as pd\nfrom PIL import Image\nfrom torchvision import models, transforms\nfrom torch.utils.data import DataLoader, Dataset\nimport tensorflow as tf\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator, load_img, img_to_array\nimport matplotlib.pyplot as plt\nimport cv2\nimport random\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.applications.resnet50 import ResNet50\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.layers import Dense, GlobalAveragePooling2D\nimport seaborn as sns\nfrom skimage.feature import local_binary_pattern\nfrom skimage.feature import graycomatrix, graycoprops\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.metrics import precision_score, recall_score, accuracy_score\nimport numpy as np\nfrom itertools import chain\nimport yaml","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2024-12-09T23:01:57.483183Z","iopub.execute_input":"2024-12-09T23:01:57.483582Z","iopub.status.idle":"2024-12-09T23:02:17.727493Z","shell.execute_reply.started":"2024-12-09T23:01:57.483539Z","shell.execute_reply":"2024-12-09T23:02:17.726042Z"}},"outputs":[],"execution_count":1},{"cell_type":"code","source":"import yaml\n\n# Load config.yaml\nwith open(\"./config.yaml\", \"r\") as file:\n    config = yaml.safe_load(file)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-09T23:02:17.729608Z","iopub.execute_input":"2024-12-09T23:02:17.730287Z","iopub.status.idle":"2024-12-09T23:02:17.742526Z","shell.execute_reply.started":"2024-12-09T23:02:17.730250Z","shell.execute_reply":"2024-12-09T23:02:17.741417Z"}},"outputs":[],"execution_count":2},{"cell_type":"markdown","source":"**Data preprocessing and visualization**","metadata":{}},{"cell_type":"code","source":"excel_path = config['csv_path']\ndf = pd.read_csv(excel_path)\nprint(df.head())","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-09T23:02:17.744049Z","iopub.execute_input":"2024-12-09T23:02:17.744500Z","iopub.status.idle":"2024-12-09T23:02:17.811109Z","shell.execute_reply.started":"2024-12-09T23:02:17.744456Z","shell.execute_reply":"2024-12-09T23:02:17.809913Z"}},"outputs":[{"name":"stdout","text":"                                                name  ground truth\n0  S-2006-005094_PAS_1of2_64552732435c92704a3d37c...             0\n1  S-2006-005094_PAS_1of2_64552732435c92704a3d37c...             0\n2  S-2006-005094_PAS_1of2_64552732435c92704a3d37c...             0\n3  S-2006-005094_PAS_1of2_64552732435c92704a3d37c...             0\n4  S-2006-005094_PAS_1of2_64552732435c92704a3d37d...             0\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"#path to both the folders\nfolder1_path = config['globally_sclerotic_folder']\nfolder2_path = config['non_globally_sclerotic_folder']\nimg_paths = [folder1_path, folder2_path]\n\n#adding a new column to the dataframe that contains the path to the image and it's name\ndf['image_path'] = df['name'].apply(\n        lambda x: os.path.join(folder1_path, x) if os.path.exists(os.path.join(folder1_path, x))\n        else os.path.join(folder2_path, x) if os.path.exists(os.path.join(folder2_path, x))\n        else None\n    )\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-09T23:02:17.813460Z","iopub.execute_input":"2024-12-09T23:02:17.813809Z","iopub.status.idle":"2024-12-09T23:04:35.791423Z","shell.execute_reply.started":"2024-12-09T23:02:17.813777Z","shell.execute_reply":"2024-12-09T23:04:35.790270Z"}},"outputs":[],"execution_count":4},{"cell_type":"markdown","source":"**Custom data generator for feature extraction process**","metadata":{}},{"cell_type":"code","source":"#custom data generator to prepare the image data for batches for feature extraction process, using ImageDataGenerator\ndef custom_data_generator(df, batch_size, target_size=(224, 224), augment=False):\n    datagen = ImageDataGenerator(\n        rescale=1.0 / 255,\n        horizontal_flip=True if augment else False,\n        rotation_range=30 if augment else 0,\n        zoom_range=0.2 if augment else 0.0\n    )\n\n    while True:\n        batch_data = df.sample(n=batch_size)\n        images = []\n        labels = []\n\n        #iterating throught the batch\n        for _, row in batch_data.iterrows():\n            img = load_img(row['image_path'], target_size=target_size)\n            img_array = img_to_array(img)\n            images.append(img_array)\n            labels.append(row['ground truth'])\n\n        images = np.array(images)\n        labels = np.array(labels)\n\n        # Yield augmented images and their corresponding labels\n        yield datagen.flow(images, labels, batch_size=batch_size).__next__()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-09T23:04:35.792733Z","iopub.execute_input":"2024-12-09T23:04:35.793101Z","iopub.status.idle":"2024-12-09T23:04:35.800829Z","shell.execute_reply.started":"2024-12-09T23:04:35.793038Z","shell.execute_reply":"2024-12-09T23:04:35.799621Z"}},"outputs":[],"execution_count":5},{"cell_type":"code","source":"# Parameters\nbatch_size = 32\ntarget_size = (224, 224)\n\ntrain_generator = custom_data_generator(df, batch_size, target_size, augment=True)\n\n# Testing the generator\nimages, labels = next(train_generator)\nprint(f\"Batch image shape: {images.shape}\")  # (batch_size, 224, 224, 3)\nprint(f\"Batch labels: {labels}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-09T23:04:35.802229Z","iopub.execute_input":"2024-12-09T23:04:35.802567Z","iopub.status.idle":"2024-12-09T23:04:38.432903Z","shell.execute_reply.started":"2024-12-09T23:04:35.802535Z","shell.execute_reply":"2024-12-09T23:04:38.431815Z"}},"outputs":[{"name":"stdout","text":"Batch image shape: (32, 224, 224, 3)\nBatch labels: [0 0 1 0 1 0 0 0 0 0 0 0 0 0 0 1 0 0 0 1 0 1 1 0 0 0 0 0 0 0 0 0]\n","output_type":"stream"}],"execution_count":6},{"cell_type":"markdown","source":"**Defining the process of extraction of features from inages**","metadata":{}},{"cell_type":"code","source":"\nimport torch\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")\n\n# feature extraction functions\ndef extract_morphological_features(image):\n    _, binary_image = cv2.threshold(image, 128, 255, cv2.THRESH_BINARY_INV)\n    contours, _ = cv2.findContours(binary_image, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n    if contours:\n        largest_contour = max(contours, key=cv2.contourArea)\n        area = cv2.contourArea(largest_contour)\n        perimeter = cv2.arcLength(largest_contour, True)\n        x, y, w, h = cv2.boundingRect(largest_contour)\n        aspect_ratio = float(w) / h\n        rect_area = w * h\n        extent = float(area) / rect_area\n        return [area, perimeter, aspect_ratio, extent]\n    else:\n        return [0, 0, 0, 0]\n\ndef extract_textural_features(image):\n    radius = 3\n    n_points = 8 * radius\n\n    # Compute LBP (Local Binary Pattern)\n    lbp = local_binary_pattern(image.cpu().numpy(), n_points, radius, method='uniform')\n\n    # Compute Haralick features using GLCM (Gray Level Co-occurrence Matrix)\n    glcm = graycomatrix(image.cpu().numpy(), distances=[1], angles=[0], levels=256, symmetric=True, normed=True)\n\n    contrast = graycoprops(glcm, 'contrast')[0][0]\n    dissimilarity = graycoprops(glcm, 'dissimilarity')[0][0]\n    homogeneity = graycoprops(glcm, 'homogeneity')[0][0]\n    energy = graycoprops(glcm, 'energy')[0][0]\n    correlation = graycoprops(glcm, 'correlation')[0][0]\n\n    return [np.mean(lbp), np.var(lbp), contrast, dissimilarity, homogeneity, energy, correlation]\n\ndef process_images_in_batches(csv_path, batch_size=32):\n    # Load CSV file\n    data_df = pd.read_csv(csv_path)\n\n    data_df['image_path'] = data_df['name'].apply(\n        lambda x: os.path.join(folder1_path, x) if os.path.exists(os.path.join(folder1_path, x))\n        else os.path.join(folder2_path, x) if os.path.exists(os.path.join(folder2_path, x))\n        else None\n    )\n    # Initialize lists to store features and labels\n    features_list = []\n    labels_list = []\n\n    # Split data into batches\n    num_images = len(data_df)\n    for batch_start in range(0, num_images, batch_size):\n        batch_end = min(batch_start + batch_size, num_images)\n        batch_data = data_df.iloc[batch_start:batch_end]\n\n        print(f\"Processing batch {batch_start // batch_size + 1}/{(num_images + batch_size - 1) // batch_size}...\")\n\n        for _, row in batch_data.iterrows():\n            image_path = row['image_path']  #column contains full paths to images\n            label = row['ground truth']  #column contains the class label\n\n            # Read image in grayscale\n            image = cv2.imread(image_path, cv2.IMREAD_GRAYSCALE)\n\n            # Check if the image was loaded successfully\n            if image is None:\n                print(f\"Warning: Unable to load image at {image_path}. Skipping...\")\n                continue\n\n            # Move image to GPU as a tensor\n            image_tensor = torch.tensor(image).to(device)\n\n            # Extract features (morphological and textural)\n            morph_features = extract_morphological_features(image)\n            textural_features = extract_textural_features(image_tensor)\n\n            # Combine features and append to list\n            features_list.append(morph_features + textural_features)\n            labels_list.append(label)\n\n        print(f\"Batch {batch_start // batch_size + 1} completed.\")\n\n    return np.array(features_list), np.array(labels_list)\n\ncsv_path = config['csv_path']\n\n# Process images in batches and extract features\nfeatures, labels = process_images_in_batches(csv_path)\n\n# Save extracted features for later use\nnp.save('features.npy', features)\nnp.save('labels.npy', labels)\n\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-09T23:04:38.434596Z","iopub.execute_input":"2024-12-09T23:04:38.434957Z","iopub.status.idle":"2024-12-09T23:33:19.326101Z","shell.execute_reply.started":"2024-12-09T23:04:38.434923Z","shell.execute_reply":"2024-12-09T23:33:19.324254Z"}},"outputs":[{"name":"stdout","text":"Using device: cpu\nProcessing batch 1/180...\nBatch 1 completed.\nProcessing batch 2/180...\nBatch 2 completed.\nProcessing batch 3/180...\nBatch 3 completed.\nProcessing batch 4/180...\nBatch 4 completed.\nProcessing batch 5/180...\nBatch 5 completed.\nProcessing batch 6/180...\nBatch 6 completed.\nProcessing batch 7/180...\nBatch 7 completed.\nProcessing batch 8/180...\nBatch 8 completed.\nProcessing batch 9/180...\nBatch 9 completed.\nProcessing batch 10/180...\nBatch 10 completed.\nProcessing batch 11/180...\nBatch 11 completed.\nProcessing batch 12/180...\nBatch 12 completed.\nProcessing batch 13/180...\nBatch 13 completed.\nProcessing batch 14/180...\nBatch 14 completed.\nProcessing batch 15/180...\nBatch 15 completed.\nProcessing batch 16/180...\nBatch 16 completed.\nProcessing batch 17/180...\nBatch 17 completed.\nProcessing batch 18/180...\nBatch 18 completed.\nProcessing batch 19/180...\nBatch 19 completed.\nProcessing batch 20/180...\nBatch 20 completed.\nProcessing batch 21/180...\nBatch 21 completed.\nProcessing batch 22/180...\nBatch 22 completed.\nProcessing batch 23/180...\nBatch 23 completed.\nProcessing batch 24/180...\nBatch 24 completed.\nProcessing batch 25/180...\nBatch 25 completed.\nProcessing batch 26/180...\nBatch 26 completed.\nProcessing batch 27/180...\nBatch 27 completed.\nProcessing batch 28/180...\nBatch 28 completed.\nProcessing batch 29/180...\nBatch 29 completed.\nProcessing batch 30/180...\nBatch 30 completed.\nProcessing batch 31/180...\nBatch 31 completed.\nProcessing batch 32/180...\nBatch 32 completed.\nProcessing batch 33/180...\nBatch 33 completed.\nProcessing batch 34/180...\nBatch 34 completed.\nProcessing batch 35/180...\nBatch 35 completed.\nProcessing batch 36/180...\nBatch 36 completed.\nProcessing batch 37/180...\nBatch 37 completed.\nProcessing batch 38/180...\nBatch 38 completed.\nProcessing batch 39/180...\nBatch 39 completed.\nProcessing batch 40/180...\nBatch 40 completed.\nProcessing batch 41/180...\nBatch 41 completed.\nProcessing batch 42/180...\nBatch 42 completed.\nProcessing batch 43/180...\nBatch 43 completed.\nProcessing batch 44/180...\nBatch 44 completed.\nProcessing batch 45/180...\nBatch 45 completed.\nProcessing batch 46/180...\nBatch 46 completed.\nProcessing batch 47/180...\nBatch 47 completed.\nProcessing batch 48/180...\nBatch 48 completed.\nProcessing batch 49/180...\nBatch 49 completed.\nProcessing batch 50/180...\nBatch 50 completed.\nProcessing batch 51/180...\nBatch 51 completed.\nProcessing batch 52/180...\nBatch 52 completed.\nProcessing batch 53/180...\nBatch 53 completed.\nProcessing batch 54/180...\nBatch 54 completed.\nProcessing batch 55/180...\nBatch 55 completed.\nProcessing batch 56/180...\nBatch 56 completed.\nProcessing batch 57/180...\nBatch 57 completed.\nProcessing batch 58/180...\nBatch 58 completed.\nProcessing batch 59/180...\nBatch 59 completed.\nProcessing batch 60/180...\nBatch 60 completed.\nProcessing batch 61/180...\nBatch 61 completed.\nProcessing batch 62/180...\nBatch 62 completed.\nProcessing batch 63/180...\nBatch 63 completed.\nProcessing batch 64/180...\nBatch 64 completed.\nProcessing batch 65/180...\nBatch 65 completed.\nProcessing batch 66/180...\nBatch 66 completed.\nProcessing batch 67/180...\nBatch 67 completed.\nProcessing batch 68/180...\nBatch 68 completed.\nProcessing batch 69/180...\nBatch 69 completed.\nProcessing batch 70/180...\nBatch 70 completed.\nProcessing batch 71/180...\nBatch 71 completed.\nProcessing batch 72/180...\nBatch 72 completed.\nProcessing batch 73/180...\nBatch 73 completed.\nProcessing batch 74/180...\nBatch 74 completed.\nProcessing batch 75/180...\nBatch 75 completed.\nProcessing batch 76/180...\nBatch 76 completed.\nProcessing batch 77/180...\nBatch 77 completed.\nProcessing batch 78/180...\nBatch 78 completed.\nProcessing batch 79/180...\nBatch 79 completed.\nProcessing batch 80/180...\nBatch 80 completed.\nProcessing batch 81/180...\nBatch 81 completed.\nProcessing batch 82/180...\nBatch 82 completed.\nProcessing batch 83/180...\nBatch 83 completed.\nProcessing batch 84/180...\nBatch 84 completed.\nProcessing batch 85/180...\nBatch 85 completed.\nProcessing batch 86/180...\nBatch 86 completed.\nProcessing batch 87/180...\nBatch 87 completed.\nProcessing batch 88/180...\nBatch 88 completed.\nProcessing batch 89/180...\nBatch 89 completed.\nProcessing batch 90/180...\nBatch 90 completed.\nProcessing batch 91/180...\nBatch 91 completed.\nProcessing batch 92/180...\nBatch 92 completed.\nProcessing batch 93/180...\nBatch 93 completed.\nProcessing batch 94/180...\nBatch 94 completed.\nProcessing batch 95/180...\nBatch 95 completed.\nProcessing batch 96/180...\nBatch 96 completed.\nProcessing batch 97/180...\nBatch 97 completed.\nProcessing batch 98/180...\nBatch 98 completed.\nProcessing batch 99/180...\nBatch 99 completed.\nProcessing batch 100/180...\nBatch 100 completed.\nProcessing batch 101/180...\nBatch 101 completed.\nProcessing batch 102/180...\nBatch 102 completed.\nProcessing batch 103/180...\nBatch 103 completed.\nProcessing batch 104/180...\nBatch 104 completed.\nProcessing batch 105/180...\nBatch 105 completed.\nProcessing batch 106/180...\nBatch 106 completed.\nProcessing batch 107/180...\nBatch 107 completed.\nProcessing batch 108/180...\nBatch 108 completed.\nProcessing batch 109/180...\nBatch 109 completed.\nProcessing batch 110/180...\nBatch 110 completed.\nProcessing batch 111/180...\nBatch 111 completed.\nProcessing batch 112/180...\nBatch 112 completed.\nProcessing batch 113/180...\nBatch 113 completed.\nProcessing batch 114/180...\nBatch 114 completed.\nProcessing batch 115/180...\nBatch 115 completed.\nProcessing batch 116/180...\nBatch 116 completed.\nProcessing batch 117/180...\nBatch 117 completed.\nProcessing batch 118/180...\nBatch 118 completed.\nProcessing batch 119/180...\nBatch 119 completed.\nProcessing batch 120/180...\nBatch 120 completed.\nProcessing batch 121/180...\nBatch 121 completed.\nProcessing batch 122/180...\nBatch 122 completed.\nProcessing batch 123/180...\nBatch 123 completed.\nProcessing batch 124/180...\nBatch 124 completed.\nProcessing batch 125/180...\nBatch 125 completed.\nProcessing batch 126/180...\nBatch 126 completed.\nProcessing batch 127/180...\nBatch 127 completed.\nProcessing batch 128/180...\nBatch 128 completed.\nProcessing batch 129/180...\nBatch 129 completed.\nProcessing batch 130/180...\nBatch 130 completed.\nProcessing batch 131/180...\nBatch 131 completed.\nProcessing batch 132/180...\nBatch 132 completed.\nProcessing batch 133/180...\nBatch 133 completed.\nProcessing batch 134/180...\nBatch 134 completed.\nProcessing batch 135/180...\nBatch 135 completed.\nProcessing batch 136/180...\nBatch 136 completed.\nProcessing batch 137/180...\nBatch 137 completed.\nProcessing batch 138/180...\nBatch 138 completed.\nProcessing batch 139/180...\nBatch 139 completed.\nProcessing batch 140/180...\nBatch 140 completed.\nProcessing batch 141/180...\nBatch 141 completed.\nProcessing batch 142/180...\nBatch 142 completed.\nProcessing batch 143/180...\nBatch 143 completed.\nProcessing batch 144/180...\nBatch 144 completed.\nProcessing batch 145/180...\nBatch 145 completed.\nProcessing batch 146/180...\nBatch 146 completed.\nProcessing batch 147/180...\nBatch 147 completed.\nProcessing batch 148/180...\nBatch 148 completed.\nProcessing batch 149/180...\nBatch 149 completed.\nProcessing batch 150/180...\nBatch 150 completed.\nProcessing batch 151/180...\nBatch 151 completed.\nProcessing batch 152/180...\nBatch 152 completed.\nProcessing batch 153/180...\nBatch 153 completed.\nProcessing batch 154/180...\nBatch 154 completed.\nProcessing batch 155/180...\nBatch 155 completed.\nProcessing batch 156/180...\nBatch 156 completed.\nProcessing batch 157/180...\nBatch 157 completed.\nProcessing batch 158/180...\nBatch 158 completed.\nProcessing batch 159/180...\nBatch 159 completed.\nProcessing batch 160/180...\nBatch 160 completed.\nProcessing batch 161/180...\nBatch 161 completed.\nProcessing batch 162/180...\nBatch 162 completed.\nProcessing batch 163/180...\nBatch 163 completed.\nProcessing batch 164/180...\nBatch 164 completed.\nProcessing batch 165/180...\nBatch 165 completed.\nProcessing batch 166/180...\nBatch 166 completed.\nProcessing batch 167/180...\nBatch 167 completed.\nProcessing batch 168/180...\nBatch 168 completed.\nProcessing batch 169/180...\nBatch 169 completed.\nProcessing batch 170/180...\nBatch 170 completed.\nProcessing batch 171/180...\nBatch 171 completed.\nProcessing batch 172/180...\nBatch 172 completed.\nProcessing batch 173/180...\nBatch 173 completed.\nProcessing batch 174/180...\nBatch 174 completed.\nProcessing batch 175/180...\nBatch 175 completed.\nProcessing batch 176/180...\nBatch 176 completed.\nProcessing batch 177/180...\nBatch 177 completed.\nProcessing batch 178/180...\nBatch 178 completed.\nProcessing batch 179/180...\nBatch 179 completed.\nProcessing batch 180/180...\nBatch 180 completed.\n","output_type":"stream"}],"execution_count":7},{"cell_type":"markdown","source":"**Data processing on the extracted features**","metadata":{}},{"cell_type":"code","source":"df_array = pd.DataFrame(features, columns=[f'f{i}' for i in range(1,12)])   #converting the features.npy file into a dataframe\ndf = pd.concat([df, df_array], ignore_index=True,axis=1)                    #concatenating the df_array and df dataframes\n\nprint(df.shape)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-09T23:33:19.328321Z","iopub.execute_input":"2024-12-09T23:33:19.328927Z","iopub.status.idle":"2024-12-09T23:33:19.343825Z","shell.execute_reply.started":"2024-12-09T23:33:19.328873Z","shell.execute_reply":"2024-12-09T23:33:19.342348Z"}},"outputs":[{"name":"stdout","text":"(5758, 14)\n","output_type":"stream"}],"execution_count":8},{"cell_type":"code","source":"#renaming the columns for better interpretability\ndf.rename(columns={0: 'name', 1: 'ground truth',2:'image_path'}, inplace=True)\ndf.rename(columns={3: 'f1', 4: 'f2',5:'f3',6:'f4',7:'f5',8:'f6',9:'f7',10:'f8',11:'f9',12:'f10',13:'f11'}, inplace=True)\n\n#normalizing some of the columns\nscaler = MinMaxScaler()\ndf[['f1', 'f2','f5','f6','f7']] = scaler.fit_transform(df[['f1', 'f2','f5','f6','f7']])\ndf.head()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-09T23:33:19.345471Z","iopub.execute_input":"2024-12-09T23:33:19.345928Z","iopub.status.idle":"2024-12-09T23:33:19.403329Z","shell.execute_reply.started":"2024-12-09T23:33:19.345893Z","shell.execute_reply":"2024-12-09T23:33:19.402124Z"}},"outputs":[{"execution_count":9,"output_type":"execute_result","data":{"text/plain":"                                                name  ground truth  \\\n0  S-2006-005094_PAS_1of2_64552732435c92704a3d37c...             0   \n1  S-2006-005094_PAS_1of2_64552732435c92704a3d37c...             0   \n2  S-2006-005094_PAS_1of2_64552732435c92704a3d37c...             0   \n3  S-2006-005094_PAS_1of2_64552732435c92704a3d37c...             0   \n4  S-2006-005094_PAS_1of2_64552732435c92704a3d37d...             0   \n\n                                          image_path        f1        f2  \\\n0  /kaggle/input/glomeruloscelorosis/non_globally...  0.135396  0.172140   \n1  /kaggle/input/glomeruloscelorosis/non_globally...  0.106163  0.262284   \n2  /kaggle/input/glomeruloscelorosis/non_globally...  0.160521  0.065619   \n3  /kaggle/input/glomeruloscelorosis/non_globally...  0.027140  0.088588   \n4  /kaggle/input/glomeruloscelorosis/non_globally...  0.093153  0.319723   \n\n         f3        f4        f5        f6        f7        f8        f9  \\\n0  0.613532  0.460757  0.540810  0.679090  0.306279  7.813146  0.306785   \n1  0.526870  0.355202  0.375573  0.677076  0.302721  9.253420  0.155894   \n2  1.155963  0.742209  0.371169  0.655720  0.197254  7.287852  0.222909   \n3  0.787671  0.285193  0.372979  0.657166  0.320763  9.172040  0.175850   \n4  0.902439  0.243723  0.396571  0.670934  0.291144  8.801991  0.179107   \n\n        f10       f11  \n0  0.072380  0.980402  \n1  0.016113  0.967392  \n2  0.036135  0.982127  \n3  0.021963  0.970531  \n4  0.022915  0.970858  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>name</th>\n      <th>ground truth</th>\n      <th>image_path</th>\n      <th>f1</th>\n      <th>f2</th>\n      <th>f3</th>\n      <th>f4</th>\n      <th>f5</th>\n      <th>f6</th>\n      <th>f7</th>\n      <th>f8</th>\n      <th>f9</th>\n      <th>f10</th>\n      <th>f11</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>S-2006-005094_PAS_1of2_64552732435c92704a3d37c...</td>\n      <td>0</td>\n      <td>/kaggle/input/glomeruloscelorosis/non_globally...</td>\n      <td>0.135396</td>\n      <td>0.172140</td>\n      <td>0.613532</td>\n      <td>0.460757</td>\n      <td>0.540810</td>\n      <td>0.679090</td>\n      <td>0.306279</td>\n      <td>7.813146</td>\n      <td>0.306785</td>\n      <td>0.072380</td>\n      <td>0.980402</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>S-2006-005094_PAS_1of2_64552732435c92704a3d37c...</td>\n      <td>0</td>\n      <td>/kaggle/input/glomeruloscelorosis/non_globally...</td>\n      <td>0.106163</td>\n      <td>0.262284</td>\n      <td>0.526870</td>\n      <td>0.355202</td>\n      <td>0.375573</td>\n      <td>0.677076</td>\n      <td>0.302721</td>\n      <td>9.253420</td>\n      <td>0.155894</td>\n      <td>0.016113</td>\n      <td>0.967392</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>S-2006-005094_PAS_1of2_64552732435c92704a3d37c...</td>\n      <td>0</td>\n      <td>/kaggle/input/glomeruloscelorosis/non_globally...</td>\n      <td>0.160521</td>\n      <td>0.065619</td>\n      <td>1.155963</td>\n      <td>0.742209</td>\n      <td>0.371169</td>\n      <td>0.655720</td>\n      <td>0.197254</td>\n      <td>7.287852</td>\n      <td>0.222909</td>\n      <td>0.036135</td>\n      <td>0.982127</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>S-2006-005094_PAS_1of2_64552732435c92704a3d37c...</td>\n      <td>0</td>\n      <td>/kaggle/input/glomeruloscelorosis/non_globally...</td>\n      <td>0.027140</td>\n      <td>0.088588</td>\n      <td>0.787671</td>\n      <td>0.285193</td>\n      <td>0.372979</td>\n      <td>0.657166</td>\n      <td>0.320763</td>\n      <td>9.172040</td>\n      <td>0.175850</td>\n      <td>0.021963</td>\n      <td>0.970531</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>S-2006-005094_PAS_1of2_64552732435c92704a3d37d...</td>\n      <td>0</td>\n      <td>/kaggle/input/glomeruloscelorosis/non_globally...</td>\n      <td>0.093153</td>\n      <td>0.319723</td>\n      <td>0.902439</td>\n      <td>0.243723</td>\n      <td>0.396571</td>\n      <td>0.670934</td>\n      <td>0.291144</td>\n      <td>8.801991</td>\n      <td>0.179107</td>\n      <td>0.022915</td>\n      <td>0.970858</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"execution_count":9},{"cell_type":"code","source":"nan_rows = df[df.isna().any(axis=1)]\n\n# Display rows containing NaN values\nprint(nan_rows)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-09T23:33:19.406746Z","iopub.execute_input":"2024-12-09T23:33:19.407132Z","iopub.status.idle":"2024-12-09T23:33:19.419356Z","shell.execute_reply.started":"2024-12-09T23:33:19.407086Z","shell.execute_reply":"2024-12-09T23:33:19.418133Z"}},"outputs":[{"name":"stdout","text":"Empty DataFrame\nColumns: [name, ground truth, image_path, f1, f2, f3, f4, f5, f6, f7, f8, f9, f10, f11]\nIndex: []\n","output_type":"stream"}],"execution_count":10},{"cell_type":"markdown","source":"**Defining a custom data generator for loading both types of data**","metadata":{}},{"cell_type":"code","source":"# Custom Dataset Class\nclass CustomDataset(Dataset):\n    def __init__(self, df, target_size=(224,224), augment=False):\n        self.df=df\n        self.target_size=target_size\n        \n        self.label_counts = self.df['ground truth'].value_counts().to_dict()\n        self.label_glomeruli = self.label_counts.get(1, 0)  # Count of 1 instances\n        self.label_non_glomeruli = self.label_counts.get(0, 0)\n\n        #transformations\n        self.transform=transforms.Compose([\n            transforms.Resize(self.target_size),\n            transforms.ToTensor(),\n            transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229,0.224,0.225])\n        ])\n        \n        #implementing data augmentation to increase the number of samples\n        if augment==False:\n            self.augment_transform= transforms.Compose([\n                transforms.RandomHorizontalFlip(),\n                transforms.RandomRotation(30),\n                self.random_zoom(),\n                self.transform\n            ])\n\n    def random_zoom(self):\n        return transforms.RandomAffine(\n            degrees=0,           # No rotation\n            translate=None,      # No translation\n            scale=(0.8, 1.2)     # Random scaling (zoom in or out)\n        )\n\n    def __len__(self):\n        return len(self.df)\n\n    def __getitem__(self, idx):\n        row=self.df.iloc[idx]\n\n        # Load image\n        img_name= row['name']\n        img=Image.open(row['image_path'])\n\n        # Convert RGBA to RGB if necessary\n        if img.mode=='RGBA':\n            img=img.convert('RGB')\n            \n        # Apply augmentations if specified\n        img=self.augment_transform(img)\n\n        # Extract textual features\n        feature_values = row[['f1', 'f2', 'f3', 'f4', 'f5', 'f6', 'f7', 'f8', 'f9', 'f10', 'f11']].values\n        feature_values = np.array(feature_values, dtype=np.float32)\n        \n        # Extract label\n        label=float(row['ground truth'])\n      \n        return img, feature_values, label, self.label_glomeruli, self.label_non_glomeruli, img_name\n\ntest_dataset = CustomDataset(df, target_size=(224, 224), augment=False)\ntest_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n\n#loop over DataLoader\nfor images, features, labels,count_glomeruli, count_non_glomeruli, img_name in test_loader:\n    print(f\"Batch images shape: {images.shape}\")\n    print(f\"Batch features shape: {features.shape}\")\n    print(f\"Batch labels shape: {labels.shape}\")\n    print(count_glomeruli.dtype)\n    break  ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-09T23:33:19.420934Z","iopub.execute_input":"2024-12-09T23:33:19.421328Z","iopub.status.idle":"2024-12-09T23:33:21.709823Z","shell.execute_reply.started":"2024-12-09T23:33:19.421296Z","shell.execute_reply":"2024-12-09T23:33:21.708547Z"}},"outputs":[{"name":"stdout","text":"Batch images shape: torch.Size([32, 3, 224, 224])\nBatch features shape: torch.Size([32, 11])\nBatch labels shape: torch.Size([32])\ntorch.int64\n","output_type":"stream"}],"execution_count":11},{"cell_type":"code","source":"#defining the model that will take both the images and features as input, by extending the nn.Module class\nclass MultiInputModel(nn.Module):\n    def __init__(self, image_input_shape, feature_input_shape):\n        super(MultiInputModel, self).__init__()\n\n        # Image input branch \n        self.image_fc=nn.Sequential(\n            nn.Flatten(),\n        )\n\n        # Feature input branch \n        self.feature_fc=nn.Sequential(\n            nn.Linear(feature_input_shape, 64),\n            nn.ReLU(),\n            nn.Dropout(0.2)\n        )\n\n        # Combine both branches\n        self.combined_fc=nn.Sequential(\n            nn.Linear(224*224*3+64, 64),\n            nn.ReLU(),\n            nn.Dropout(0.2)\n        )\n        \n        # Output layer (for binary classification)\n        self.output_fc=nn.Sequential(\n            nn.Linear(64,2),\n            nn.Sigmoid()\n        )\n\n    def forward(self, image_input, feature_input):\n        x = self.image_fc(image_input)  # Process the image input\n        y = self.feature_fc(feature_input)  # Process the feature input\n        \n        # Concatenate both branches\n        combined = torch.cat((x, y), dim=1)\n        \n        # Forward pass through the combined layers\n        z = self.combined_fc(combined)\n        \n        # Output prediction\n        output = self.output_fc(z)\n        \n        return output\n\n\nimage_input_shape=(224, 224, 3)\n\ncriterion = nn.CrossEntropyLoss()\n\nfeature_input_shape=11\n\nmodel=MultiInputModel(image_input_shape, feature_input_shape)\nprint(model)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-09T23:33:21.711728Z","iopub.execute_input":"2024-12-09T23:33:21.712225Z","iopub.status.idle":"2024-12-09T23:33:21.848362Z","shell.execute_reply.started":"2024-12-09T23:33:21.712173Z","shell.execute_reply":"2024-12-09T23:33:21.847150Z"}},"outputs":[{"name":"stdout","text":"MultiInputModel(\n  (image_fc): Sequential(\n    (0): Flatten(start_dim=1, end_dim=-1)\n  )\n  (feature_fc): Sequential(\n    (0): Linear(in_features=11, out_features=64, bias=True)\n    (1): ReLU()\n    (2): Dropout(p=0.2, inplace=False)\n  )\n  (combined_fc): Sequential(\n    (0): Linear(in_features=150592, out_features=64, bias=True)\n    (1): ReLU()\n    (2): Dropout(p=0.2, inplace=False)\n  )\n  (output_fc): Sequential(\n    (0): Linear(in_features=64, out_features=2, bias=True)\n    (1): Sigmoid()\n  )\n)\n","output_type":"stream"}],"execution_count":12},{"cell_type":"code","source":"model_path=config['model_path']       #loading the model parameters\nmodel.load_state_dict(torch.load(model_path,map_location=torch.device('cpu')))\nmodel.eval()  ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-09T23:33:21.849533Z","iopub.execute_input":"2024-12-09T23:33:21.849871Z","iopub.status.idle":"2024-12-09T23:33:22.563617Z","shell.execute_reply.started":"2024-12-09T23:33:21.849841Z","shell.execute_reply":"2024-12-09T23:33:22.562400Z"}},"outputs":[{"name":"stderr","text":"/tmp/ipykernel_24/1413718304.py:2: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n  model.load_state_dict(torch.load(model_path,map_location=torch.device('cpu')))\n","output_type":"stream"},{"execution_count":13,"output_type":"execute_result","data":{"text/plain":"MultiInputModel(\n  (image_fc): Sequential(\n    (0): Flatten(start_dim=1, end_dim=-1)\n  )\n  (feature_fc): Sequential(\n    (0): Linear(in_features=11, out_features=64, bias=True)\n    (1): ReLU()\n    (2): Dropout(p=0.2, inplace=False)\n  )\n  (combined_fc): Sequential(\n    (0): Linear(in_features=150592, out_features=64, bias=True)\n    (1): ReLU()\n    (2): Dropout(p=0.2, inplace=False)\n  )\n  (output_fc): Sequential(\n    (0): Linear(in_features=64, out_features=2, bias=True)\n    (1): Sigmoid()\n  )\n)"},"metadata":{}}],"execution_count":13},{"cell_type":"markdown","source":"**Generating the CSV**","metadata":{}},{"cell_type":"code","source":"# Test evaluation\nimport tqdm\ndef calculate_metrics(y_true, y_pred):\n    # Convert probabilities to binary predictions (class 0 or 1)\n    y_pred_binary = torch.argmax(y_pred, dim=1).cpu().detach().numpy()  # Get the class with highest probability\n    \n    # Convert y_true to numpy\n    y_true = y_true.cpu().numpy()\n    \n    # Calculate precision and recall\n    precision = precision_score(y_true, y_pred_binary)\n    recall = recall_score(y_true, y_pred_binary)\n    accuracy=accuracy_score(y_true,y_pred_binary)\n    \n    return accuracy,precision, recall\n\ntest_loss = 0\nall_labels = []\nall_preds = []\nimg_names=[]\n\nwith torch.no_grad():\n    for images, features, labels, count_glomeruli, count_non_glomeruli, img_name in tqdm.tqdm(test_loader, desc=\"Test Evaluation\", leave=False):\n        images, features, labels = images.to(device).float(), features.to(device).float(), labels.to(device).long()\n\n        outputs = model(images, features)\n        loss = criterion(outputs, labels)\n        test_loss += loss.item()\n\n        all_labels.append(labels)\n        all_preds.append(outputs)\n        img_names.append(img_name)\n        \n# Compute test loss and metrics\ntest_loss /= len(test_loader)\nall_labels = torch.cat(all_labels)\nall_preds = torch.cat(all_preds)\ntest_accuracy, test_precision, test_recall = calculate_metrics(all_labels, all_preds)\n\n\n\n# Print test results\nprint(f\"Test Loss: {test_loss:.3f}\")\nprint(f\"Test Accuracy: {test_accuracy:.4f}\")\nprint(f\"Test Precision: {test_precision:.4f}\")\nprint(f\"Test Recall: {test_recall:.4f}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-09T23:40:02.580899Z","iopub.execute_input":"2024-12-09T23:40:02.581418Z","iopub.status.idle":"2024-12-09T23:45:10.543326Z","shell.execute_reply.started":"2024-12-09T23:40:02.581379Z","shell.execute_reply":"2024-12-09T23:45:10.542154Z"}},"outputs":[{"name":"stderr","text":"                                                                  ","output_type":"stream"},{"name":"stdout","text":"Test Loss: 0.647\nTest Accuracy: 0.8227\nTest Precision: 0.5192\nTest Recall: 0.4231\n","output_type":"stream"},{"name":"stderr","text":"\r","output_type":"stream"}],"execution_count":16},{"cell_type":"code","source":"binary_preds = torch.argmax(all_preds, dim=1).cpu().numpy()\nimg_names=list(chain.from_iterable(img_names))\n\n# Create a DataFrame for the CSV file\nresults_df = pd.DataFrame({\n    'Image Name': img_names,\n    'Prediction': binary_preds\n})\n\n# Save the DataFrame to a CSV file\nresults_df.to_csv(\"evaluation.csv\", index=False)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-09T23:45:10.545418Z","iopub.execute_input":"2024-12-09T23:45:10.545778Z","iopub.status.idle":"2024-12-09T23:45:10.566547Z","shell.execute_reply.started":"2024-12-09T23:45:10.545744Z","shell.execute_reply":"2024-12-09T23:45:10.565494Z"}},"outputs":[],"execution_count":17},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}
